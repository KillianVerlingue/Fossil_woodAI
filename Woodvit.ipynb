{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des d√©pendences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "# checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "# checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_small.pt\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "# predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n",
    "\n",
    "sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\", apply_postprocessing=False)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    # predictor.set_image(\"/home/killian/sam2\")\n",
    "    # masks, _, _ = predictor.predict(\"0\")\n",
    "    # üîπ Chemins vers le mod√®le et la configuration\n",
    "#checkpoint = \"./checkpoints/sam2_hiera_large.pt\"\n",
    "#model_cfg = \"sam2_hiera_l.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permet de geor√©f√©renc√© nos images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3855141078.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    from torchgeo.datasets import RasterDataset, stack_samplesu mod√®le\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#from torchgeo.datasets import EuroSAT100\n",
    "from torchgeo.datasets import RasterDataset, stack_samplesu mod√®le\n",
    "from torchgeo.samplers import GridGeoSampler, RandomGeoSampler\n",
    "from torchgeo.models import ResNet18_Weights, resnet18\n",
    "from torchgeo.transforms import AugmentationSequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/killian/.local/lib/python3.10/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/killian/.local/lib/python3.10/site-packages/rasterio/__init__.py:314: NotGeoreferencedWarning: The given matrix is equal to Affine.identity or its flipped counterpart. GDAL may ignore this matrix and save no geotransform without raising an error. This behavior is somewhat driver-specific.\n",
      "  dataset = writer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georeferenced TIFF saved as geo_kernel-v3f11e16ea8e2f7ea713cb7064461c49ae2c3a1199.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "def georeference_tiff(input_filename, output_filename):\n",
    "    # Open the input raster file\n",
    "    with rasterio.open(input_filename) as src:\n",
    "        # Get raster dimensions\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "        # Set an arbitrary transformation\n",
    "        transform = from_origin(0, 0, 1, 1)  # Adjust the pixel size as needed\n",
    "\n",
    "        # Create the output raster profile with an arbitrary CRS (coordinate reference system)\n",
    "        profile = src.profile\n",
    "        profile.update(transform=transform, crs='EPSG:4326')  # EPSG:4326 is WGS 84\n",
    "\n",
    "        # Modify output filename with \"geo_\" prefix\n",
    "        output_filename = output_filename.replace(os.path.basename(output_filename), \"geo_\" + os.path.basename(output_filename))\n",
    "\n",
    "        # Write to the output raster file\n",
    "        with rasterio.open(output_filename, 'w', **profile) as dst:\n",
    "            data = src.read()\n",
    "            dst.write(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python script.py input_file1.tif input_file2.tif ...\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    input_files = sys.argv[1:]\n",
    "    for input_file in input_files:\n",
    "        output_file = \"geo_\" + os.path.basename(input_file)\n",
    "        \n",
    "        georeference_tiff('/home/killian/data2025/15485/X200_15485_PB1.tif', '/home/killian/data2025/15485/X200_15485_PB1.tif')\n",
    "        print(f\"Georeferenced TIFF saved as {output_file}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on d√©finit le DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "# import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,input_raster, tile_size=640, stride=10,transforms=None):\n",
    "\n",
    "        # raster = rasterio.open(input_raster).read()\n",
    "        # raster = plt.imread(input_raster)\n",
    "        raster = tifffile.imread(input_raster)\n",
    "        \n",
    "        self.raster = raster[:,:tile_size,:]\n",
    "        print(self.raster.shape)\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.raster.shape[0]/self.tile_size)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx == 0:\n",
    "            start_y = 0\n",
    "            end_y = self.tile_size\n",
    "        \n",
    "        else:\n",
    "            start_y = idx*self.tile_size - self.stride\n",
    "            end_y = (idx+1)*self.tile_size - self.stride\n",
    "\n",
    "        data = self.raster[start_y:end_y,:,:]\n",
    "        data = data.swapaxes(0,2)\n",
    "        # data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "        # if self.transforms:\n",
    "        #     data = self.transforms(data)\n",
    "        return data\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader et dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8935, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = CustomDataset('/home/killian/data2025/15485/X200_15485_PB1.tif', tile_size=640, stride=10)\n",
    "dataset = CustomDataset('/home/killian/data2025/TGV4/X200_TGV4A_B-P.tif', tile_size=640, stride=10)\n",
    "dataloader = DataLoader(dataset,batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparametres mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametres\n",
    "model = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    points_per_side=50,  # Plus de points pour capturer les details\n",
    "    points_per_batch=50,  # Augmenter pour calculer le nbre de points pris en meme temps (/!\\ GPU)\n",
    "    pred_iou_thresh=0.5,  # Reduire pour accepter plus de mask\n",
    "    stability_score_thresh=0.70,  # Rzduire pour ne pas exclure trop de mask\n",
    "    stability_score_offset=0.8,\n",
    "    crop_n_layers=6,  # Ammeliore la segmentation des petites structures\n",
    "    box_nms_thresh=0.70,  # Eviter la suppression excessive de petite structure\n",
    "    crop_n_points_downscale_factor=1.2,  # Adapter aux images a haute resolution\n",
    "    min_mask_region_area=20.0,  # Conserver plus de petits objets\n",
    "    use_m2m=True,  # Mode avanc√© \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 13/13 - Avancement : 92.31%\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/sam2/inferences\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fichiers CSV pour sauvegarder les masks\n",
    "csv_masks_file = os.path.join(output_dir, \"mask_measurements.csv\")\n",
    "\n",
    "with open(csv_masks_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"Mask_ID\", \"Centroid_X\", \"Centroid_Y\", \"Area\", \"Equivalent_Diameter\"])\n",
    "\n",
    "size_threshold = 560  # Seuil pour filtrer les objets trop grands(0.5060 px/¬µm) donc <280¬µm\n",
    "\n",
    "# Boucle d'inf√©rence\n",
    "for i, image in enumerate(dataloader):\n",
    "    print(f\"Image {i+1}/{len(dataloader)} - Avancement : {i/len(dataloader):.2%}\", end='\\r')\n",
    "    \n",
    "    image_np = image.squeeze(0).cpu().numpy()\n",
    "    image_np = image_np.swapaxes(0, 2)\n",
    "    pred = model.generate(image_np)\n",
    "    \n",
    "    res_tensor = torch.stack([torch.tensor(m['segmentation'], dtype=torch.bool) for m in pred])\n",
    "    filtered_tensor = res_tensor[res_tensor.sum(dim=(1, 2)) <= size_threshold]\n",
    "    res_merge = filtered_tensor.any(dim=0).to(device)\n",
    "    \n",
    "    # Trier les masques de droite √† gauche selon la coordonn√©e X de leur centro√Øde\n",
    "    masks_info = []\n",
    "    for mask_id, mask_pred in enumerate(filtered_tensor):\n",
    "        mask_np = mask_pred.cpu().numpy().astype(np.uint8)\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_np, connectivity=8)\n",
    "        \n",
    "        for j in range(1, num_labels):  # On ignore le label 0 (arri√®re-plan)\n",
    "            x, y = centroids[j]\n",
    "            area = stats[j, cv2.CC_STAT_AREA]\n",
    "            equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
    "            masks_info.append((x, y, area, equivalent_diameter, mask_id))\n",
    "    700\n",
    "    # Trier les masques en fonction de X (croissant)\n",
    "    masks_info.sort(reverse=True, key=lambda m: m[0])\n",
    "    \n",
    "    # Sauvegarde des masques tri√©s\n",
    "    with open(csv_masks_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for x, y, area, equivalent_diameter, mask_id in masks_info:\n",
    "            writer.writerow([f\"Image_{i}\", mask_id, x, y, area, equivalent_diameter])\n",
    "    \n",
    "    # Sauvegarde de l'image de pr√©diction et de l'image originale\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(\"Image Originale\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(res_merge.cpu().numpy(), cmap='gray')\n",
    "    plt.title(\"Masques Pr√©dits\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f\"Image_{i}.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
