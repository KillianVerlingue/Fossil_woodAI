{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask \n",
    "        # if borders:\n",
    "        #     import cv2\n",
    "        #     contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        #     # Try to smooth contours\n",
    "        #     contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        #     cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "# checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "# predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n",
    "\n",
    "sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\", apply_postprocessing=False)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    # predictor.set_image(\"/home/killian/sam2\")\n",
    "    # masks, _, _ = predictor.predict(\"0\")\n",
    "    # üîπ Chemins vers le mod√®le et la configuration\n",
    "#checkpoint = \"./checkpoints/sam2_hiera_large.pt\"\n",
    "#model_cfg = \"sam2_hiera_l.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparam√®tres √† r√©gler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# On d√©finit le DataLoader\n",
    "class DataLoaderSegmentation(Dataset):\n",
    "    def __init__(self, folder_path, transforms=None):\n",
    "        super(DataLoaderSegmentation, self).__init__()\n",
    "        self.img_files = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
    "        # print(self.img_files)\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            self.mask_files.append(os.path.join(folder_path, f'{img_path[:-4]}_mask.png'))\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_files[index]\n",
    "        mask_path = self.mask_files[index]\n",
    "        image = Image.open(img_path)\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "        label = Image.open(mask_path)\n",
    "        label = np.array(label.convert(\"RGB\"))\n",
    "        # if self.transforms:\n",
    "        #     image = self.transforms(torch.from_numpy(image).float())\n",
    "                 \n",
    "        return image, torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permet de geor√©f√©renc√© nos images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchgeo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#from torchgeo.datasets import EuroSAT100\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchgeo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RasterDataset, stack_samples\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchgeo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridGeoSampler, RandomGeoSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchgeo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet18_Weights, resnet18\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchgeo'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#from torchgeo.datasets import EuroSAT100\n",
    "from torchgeo.datasets import RasterDataset, stack_samples\n",
    "from torchgeo.samplers import GridGeoSampler, RandomGeoSampler\n",
    "from torchgeo.models import ResNet18_Weights, resnet18\n",
    "from torchgeo.transforms import AugmentationSequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "#on d√©finit le datatloader\n",
    "class DataLoaderSegmentation(Dataset):\n",
    "    def __init__(self, folder_path, transforms=None):\n",
    "        super(DataLoaderSegmentation, self).__init__()\n",
    "        self.img_files = glob.glob(os.path.join(folder_path,'*.jpg'))\n",
    "        # print(self.img_files)\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            self.mask_files.append(os.path.join(folder_path,f'{img_path[:-4]}_mask.png') )\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            image = Image.open(img_path)\n",
    "            image = np.array(image.convert(\"RGB\"))\n",
    "            label = Image.open(mask_path)\n",
    "            label = np.array(label.convert(\"RGB\"))\n",
    "            # if self.transforms:\n",
    "            #     image = self.transforms(torch.from_numpy(image).float())\n",
    "                 \n",
    "            return image, torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "MEANS = [122.39, 118.23, 98.1]\n",
    "SDS = [39.81, 37.33, 33.04]\n",
    "\n",
    "test_transform = T.Compose([T.ConvertImageDtype(torch.float32),\n",
    "                            T.Normalize(MEANS,SDS),])\n",
    "\n",
    "\n",
    "# dataset = DataLoaderSegmentation('/home/killian/Annotations/Annotations', transforms=test_transform)\n",
    "dataset = DataLoaderSegmentation('/home/killian/Annotations/Annotations')\n",
    "\n",
    "img, mask = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametres\n",
    "model = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    points_per_side=20,  # Plus de points pour capturer les details\n",
    "    points_per_batch=30,  # Augmenter pour calculer le nbre de points pris en meme temps (/!\\ GPU)\n",
    "    pred_iou_thresh=0.7,  # Reduire pour accepter plus de mask\n",
    "    stability_score_thresh=0.80,  # Rzduire pour ne pas exclure trop de mask\n",
    "    stability_score_offset=0.8,\n",
    "    crop_n_layers=2,  # Ammeliore la segmentation des petites structures\n",
    "    box_nms_thresh=0.80,  # Eviter la suppression excessive de petite structure\n",
    "    crop_n_points_downscale_factor=2,  # Adapter aux images a haute resolution\n",
    "    min_mask_region_area=15.0,  # Conserver plus de petits objets\n",
    "    use_m2m=True,  # Mode avanc√© \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on definit une fonction pour concatener tout nos mask \n",
    "def merge_preds(pred):\n",
    "    res = torch.stack([torch.Tensor(mask['segmentation']) for mask in pred])\n",
    "    res.shape\n",
    "    res = res.any(dim=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1/100 - Avancement : 0.00%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2964631/3911576521.py:51: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  label_np = np.array(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mIoU moyen: 0.6046ancement : 99.00%\n",
      "Pr√©cision moyenne: 0.7297\n",
      "Rappel moyen: 0.7681\n",
      "F1-Score moyen: 0.7306\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# D√©finition du device\n",
    "device = \"cuda\"\n",
    "\n",
    "# D√©finition des m√©triques\n",
    "miou = torchmetrics.JaccardIndex(task='binary', num_classes=1).to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "f1_score = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/sam2/predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fichier CSV pour les m√©triques globales\n",
    "csv_file = os.path.join(output_dir, \"sam2_results.csv\")\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"IoU\", \"Precision\", \"Recall\", \"F1_Score\", \"Num_Masks\", \"Num_Masks_Label\"])\n",
    "\n",
    "# Fichier CSV pour les mesures des masques\n",
    "csv_masks_file = os.path.join(output_dir, \"mask_measurements.csv\")\n",
    "with open(csv_masks_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"Mask_ID\", \"Centroid_X\", \"Centroid_Y\", \"Area\", \"Equivalent_Diameter\"])\n",
    "\n",
    "size_threshold = 10000  # Seuil pour filtrer les objets trop grands\n",
    "\n",
    "# Variables pour les moyennes\n",
    "total_miou = total_precision = total_recall = total_f1 = total_images = 0\n",
    "\n",
    "for i, (batch, mask) in enumerate(dataloader):\n",
    "    print(f\"Image {i+1}/{len(dataloader)} - Avancement : {i/len(dataloader):.2%}\", end='\\r')\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    for idx, (image, label) in enumerate(zip(batch, mask)):\n",
    "        image_np = image.cpu().numpy()\n",
    "        pred = model.generate(image_np)\n",
    "        \n",
    "        res_tensor = torch.stack([torch.tensor(m['segmentation'], dtype=torch.bool) for m in pred])\n",
    "        filtered_tensor = res_tensor[res_tensor.sum(dim=(1, 2)) <= size_threshold]\n",
    "        res_merge = filtered_tensor.any(dim=0).to(device)\n",
    "        \n",
    "        label_np = np.array(label)\n",
    "        trach = torch.as_tensor(label_np[:, :, 2], dtype=torch.bool, device=device) if label_np.ndim == 3 and label_np.shape[-1] >= 3 else torch.as_tensor(label_np, dtype=torch.bool, device=device)\n",
    "        \n",
    "        num_masks_label = (label_np > 0).sum()\n",
    "        \n",
    "        # Calcul des m√©triques\n",
    "        img_miou = miou(res_merge, trach).item()\n",
    "        img_precision = precision(res_merge, trach).item()\n",
    "        img_recall = recall(res_merge, trach).item()\n",
    "        img_f1 = f1_score(res_merge, trach).item()\n",
    "        \n",
    "        total_miou += img_miou\n",
    "        total_precision += img_precision\n",
    "        total_recall += img_recall\n",
    "        total_f1 += img_f1\n",
    "        total_images += 1\n",
    "        \n",
    "        # Sauvegarde des r√©sultats dans le CSV\n",
    "        with open(csv_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([f\"Image_{i}_{idx}\", img_miou, img_precision, img_recall, img_f1, filtered_tensor.shape[0], num_masks_label])\n",
    "        \n",
    "        # Mesure des aires et centro√Ødes des masques\n",
    "        for mask_id, mask_pred in enumerate(filtered_tensor):\n",
    "            mask_np = mask_pred.cpu().numpy().astype(np.uint8)\n",
    "            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_np, connectivity=8)\n",
    "            \n",
    "            with open(csv_masks_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                for j in range(1, num_labels):\n",
    "                    x, y = centroids[j]\n",
    "                    area = stats[j, cv2.CC_STAT_AREA]\n",
    "                    equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
    "                    writer.writerow([f\"Image_{i}_{idx}\", mask_id, x, y, area, equivalent_diameter])\n",
    "        \n",
    "        # Sauvegarde des images\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Pr√©diction\")\n",
    "        plt.imshow(res_merge.cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Label\")\n",
    "        plt.imshow(trach.cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, f\"Image_{i}_{idx}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "if total_images > 0:\n",
    "    mean_miou = total_miou / total_images\n",
    "    mean_precision = total_precision / total_images\n",
    "    mean_recall = total_recall / total_images\n",
    "    mean_f1 = total_f1 / total_images\n",
    "    \n",
    "    print(f\"mIoU moyen: {mean_miou:.4f}\")\n",
    "    print(f\"Pr√©cision moyenne: {mean_precision:.4f}\")\n",
    "    print(f\"Rappel moyen: {mean_recall:.4f}\")\n",
    "    print(f\"F1-Score moyen: {mean_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le de seullage avec Opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mIoU moyen: 0.5587\n",
      "Pr√©cision moyenne: 0.7562\n",
      "Rappel moyen: 0.6850\n",
      "F1-Score moyen: 0.6918\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchmetrics\n",
    "import cv2\n",
    "\n",
    "# D√©finition du device\n",
    "device = \"cuda\"\n",
    "\n",
    "# D√©finition des m√©triques\n",
    "miou = torchmetrics.JaccardIndex(task='binary', num_classes=1).to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "f1_score = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/Annotations/Seuillage\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fichier CSV pour les m√©triques globales\n",
    "csv_file = os.path.join(output_dir, \"seuillage_results.csv\")\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"IoU\", \"Precision\", \"Recall\", \"F1_Score\", \"Num_Masks\", \"Num_Masks_Label\"])\n",
    "\n",
    "# Fichier CSV pour les mesures des masques\n",
    "csv_masks_file = os.path.join(output_dir, \"mask_measurements.csv\")\n",
    "with open(csv_masks_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"Mask_ID\", \"Centroid_X\", \"Centroid_Y\", \"Area\", \"Equivalent_Diameter\"])\n",
    "\n",
    "# Variables pour les moyennes\n",
    "total_miou = total_precision = total_recall = total_f1 = total_images = 0\n",
    "\n",
    "for idx, (image, mask) in enumerate(dataloader):\n",
    "    print(f\"{idx}/{len(dataloader)}\", end='\\r')\n",
    "    img_name = dataset.img_files[idx].split(\"/\")[-1]\n",
    "    \n",
    "    image = image.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    mask = mask.squeeze(0).cpu().numpy()\n",
    "    mask = mask[:,:,2] # keep only tracheid labels\n",
    "    \n",
    "    gray = np.mean(image, axis=2) if image.ndim == 3 else image\n",
    "    _, binary = cv2.threshold(gray.astype(np.uint8), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    processed = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "    processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(processed, connectivity=8)\n",
    "    \n",
    "    mask_bin = (mask > 0).astype(np.uint8)\n",
    "    \n",
    "    processed_bin = (processed > 0).astype(np.uint8)\n",
    "    \n",
    "    res_merge = torch.tensor(processed_bin, dtype=torch.bool, device=device)\n",
    "    \n",
    "    trach = torch.tensor(mask_bin, dtype=torch.bool, device=device)\n",
    "    \n",
    "    img_miou = miou(res_merge, trach).item()\n",
    "    img_precision = precision(res_merge, trach).item()\n",
    "    img_recall = recall(res_merge, trach).item()\n",
    "    img_f1 = f1_score(res_merge, trach).item()\n",
    "    \n",
    "    total_miou += img_miou\n",
    "    total_precision += img_precision\n",
    "    total_recall += img_recall\n",
    "    total_f1 += img_f1\n",
    "    total_images += 1\n",
    "    \n",
    "    num_masks_label = (mask_bin > 0).sum()\n",
    "    \n",
    "    # Sauvegarde des mesures des 3, 640, 640)masques\n",
    "    with open(csv_masks_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(1, num_labels):\n",
    "            x, y = centroids[i]\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
    "            writer.writerow([img_name, i, x, y, area, equivalent_diameter])\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Image d'origine\")\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Pr√©diction\")\n",
    "    pred_image = res_merge.cpu().numpy()\n",
    "    for centroid in centroids:\n",
    "        plt.scatter(centroid[0], centroid[1], color='red', s=50, marker='x')\n",
    "    plt.imshow(pred_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Label\")\n",
    "    plt.imshow(trach.cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f\"Image_{idx}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "if total_images > 0:\n",
    "    print(f\"mIoU moyen: {total_miou / total_images:.4f}\")\n",
    "    print(f\"Pr√©cision moyenne: {total_precision / total_images:.4f}\")\n",
    "    print(f\"Rappel moyen: {total_recall / total_images:.4f}\")\n",
    "    print(f\"F1-Score moyen: {total_f1 / total_images:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB3_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB3_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB3_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB4_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB4_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB4_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB5_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB5_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB5_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB4_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB4_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB4_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB3_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB3_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB3_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB5_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB5_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB5_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB3_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB3_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB3_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB6_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB6_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB6_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB4_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB4_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB4_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB2_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV4D_B-P_1.tif : Image size (395460844 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4E_B-P_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4E_B-P_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4E_B-P_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4A_B_P_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4A_B_P_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4A_B_P_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4F_B_P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4F_B_P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4F_B_P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4D_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4D_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4D_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4C_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4C_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4C_B-P_2_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV4C_B-P_1.tif : Image size (444658205 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_1_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV5B_B-P_2.tif : Image size (396766050 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5C_B-P_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5C_B-P_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5C_B-P_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_1_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV5B_B-P_1.tif : Image size (421885968 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS>=178956970\n",
    "\n",
    "extraire_bandes_images('/home/killian/data2025/11478')\n",
    "extraire_bandes_images('/home/killian/data2025/13823')\n",
    "extraire_bandes_images('/home/killian/data2025/15485')\n",
    "extraire_bandes_images('/home/killian/data2025/15492')\n",
    "extraire_bandes_images('/home/killian/data2025/TGV4')\n",
    "extraire_bandes_images('/home/killian/data2025/TGV5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
