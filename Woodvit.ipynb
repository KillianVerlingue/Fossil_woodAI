{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask \n",
    "        # if borders:\n",
    "        #     import cv2\n",
    "        #     contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        #     # Try to smooth contours\n",
    "        #     contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        #     cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "# checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "# predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n",
    "\n",
    "sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\", apply_postprocessing=False)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    # predictor.set_image(\"/home/killian/sam2\")\n",
    "    # masks, _, _ = predictor.predict(\"0\")\n",
    "    # üîπ Chemins vers le mod√®le et la configuration\n",
    "#checkpoint = \"./checkpoints/sam2_hiera_large.pt\"\n",
    "#model_cfg = \"sam2_hiera_l.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparam√®tres √† r√©gler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os  # N√©cessaire pour os.path.join\n",
    "import numpy as np  # Pour convertir en numpy array\n",
    "from PIL import Image  # Pour ouvrir les images\n",
    "\n",
    "# on d√©finit le datatloader\n",
    "class DataLoaderSegmentation(Dataset):\n",
    "\n",
    "    def __init__(self, folder_path, transforms=None):\n",
    "        super(DataLoaderSegmentation, self).__init__()\n",
    "\n",
    "        self.img_files = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
    "\n",
    "        # Print des fichiers pour d√©bogage\n",
    "        # print(self.img_files)\n",
    "        self.mask_files = []\n",
    "\n",
    "        for img_path in self.img_files:\n",
    "            # Cr√©er le chemin pour le masque correspondant\n",
    "            self.mask_files.append(os.path.join(folder_path, f'{img_path[:-4]}_mask.png'))\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_files[index]\n",
    "        mask_path = self.mask_files[index]\n",
    "\n",
    "        # Ouvrir l'image et la convertir en RGB\n",
    "        image = Image.open(img_path)\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # Ouvrir le masque et le convertir en RGB\n",
    "        label = Image.open(mask_path)\n",
    "        label = np.array(label.convert(\"RGB\"))\n",
    "\n",
    "        # Si des transformations sont sp√©cifi√©es, les appliquer (non activ√©es ici)\n",
    "        if self.transforms:\n",
    "            #  image = self.transforms(torch.from_numpy(image).float())\n",
    "              image = self.transforms(torch.as_tensor(np.array(image).astype('float')))\n",
    "\n",
    "\n",
    "        # Retourner l'image et le masque en tant que tenseurs PyTorch\n",
    "        return image, torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permet de geor√©f√©renc√© nos images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/killian/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f21cc04ca50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#from torchgeo.datasets import EuroSAT100\n",
    "from torchgeo.datasets import RasterDataset, stack_samples\n",
    "from torchgeo.samplers import GridGeoSampler, RandomGeoSampler\n",
    "from torchgeo.models import ResNet18_Weights, resnet18\n",
    "from torchgeo.transforms import AugmentationSequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/killian/.local/lib/python3.10/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/killian/.local/lib/python3.10/site-packages/rasterio/__init__.py:314: NotGeoreferencedWarning: The given matrix is equal to Affine.identity or its flipped counterpart. GDAL may ignore this matrix and save no geotransform without raising an error. This behavior is somewhat driver-specific.\n",
      "  dataset = writer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georeferenced TIFF saved as geo_kernel-v3f11e16ea8e2f7ea713cb7064461c49ae2c3a1199.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "def georeference_tiff(input_filename, output_filename):\n",
    "    # Open the input raster file\n",
    "    with rasterio.open(input_filename) as src:\n",
    "        # Get raster dimensions\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "        # Set an arbitrary transformation\n",
    "        transform = from_origin(0, 0, 1, 1)  # Adjust the pixel size as needed\n",
    "\n",
    "        # Create the output raster profile with an arbitrary CRS (coordinate reference system)\n",
    "        profile = src.profile\n",
    "        profile.update(transform=transform, crs='EPSG:4326')  # EPSG:4326 is WGS 84\n",
    "\n",
    "        # Modify output filename with \"geo_\" prefix\n",
    "        output_filename = output_filename.replace(os.path.basename(output_filename), \"geo_\" + os.path.basename(output_filename))\n",
    "\n",
    "        # Write to the output raster file\n",
    "        with rasterio.open(output_filename, 'w', **profile) as dst:\n",
    "            data = src.read()\n",
    "            dst.write(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python script.py input_file1.tif input_file2.tif ...\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    input_files = sys.argv[1:]\n",
    "    for input_file in input_files:\n",
    "        output_file = \"geo_\" + os.path.basename(input_file)\n",
    "        \n",
    "        georeference_tiff('/home/killian/data2025/15485/X200_15485_PB1.tif', '/home/killian/data2025/15485/X200_15485_PB1.tif')\n",
    "        print(f\"Georeferenced TIFF saved as {output_file}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MEANS = [122.39, 118.23, 98.1]\n",
    "SDS = [39.81, 37.33, 33.04]\n",
    "\n",
    "test_transform = T.Compose([T.ConvertImageDtype(torch.float32),\n",
    "                            T.Normalize(MEANS,SDS),])\n",
    "\n",
    "\n",
    "dataset = DataLoaderSegmentation('/home/killian/Annotations/Annotations', transforms=test_transform)\n",
    "# dataset = DataLoaderSegmentation('/home/killian/Annotations/Annotations')\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametres\n",
    "model = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    points_per_side=20,  # Plus de points pour capturer les details\n",
    "    points_per_batch=30,  # Augmenter pour calculer le nbre de points pris en meme temps (/!\\ GPU)\n",
    "    pred_iou_thresh=0.7,  # Reduire pour accepter plus de mask\n",
    "    stability_score_thresh=0.80,  # Rzduire pour ne pas exclure trop de mask\n",
    "    stability_score_offset=0.8,\n",
    "    crop_n_layers=2,  # Ammeliore la segmentation des petites structures\n",
    "    box_nms_thresh=0.80,  # Eviter la suppression excessive de petite structure\n",
    "    crop_n_points_downscale_factor=2,  # Adapter aux images a haute resolution\n",
    "    min_mask_region_area=15.0,  # Conserver plus de petits objets\n",
    "    use_m2m=True,  # Mode avanc√© \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on definit une fonction pour concatener tout nos mask \n",
    "def merge_preds(pred):\n",
    "    res = torch.stack([torch.Tensor(mask['segmentation']) for mask in pred])\n",
    "    res.shape\n",
    "    res = res.any(dim=0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Variables pour les moyennes\u001b[39;00m\n\u001b[1;32m     37\u001b[0m total_miou \u001b[38;5;241m=\u001b[39m total_precision \u001b[38;5;241m=\u001b[39m total_recall \u001b[38;5;241m=\u001b[39m total_f1 \u001b[38;5;241m=\u001b[39m total_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader\u001b[49m):\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Avancement : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# D√©finition du device\n",
    "device = \"cuda\"\n",
    "\n",
    "# D√©finition des m√©triques\n",
    "miou = torchmetrics.JaccardIndex(task='binary', num_classes=1).to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "f1_score = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/sam2/predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fichier CSV pour les m√©triques globales\n",
    "csv_file = os.path.join(output_dir, \"sam2_results.csv\")\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"IoU\", \"Precision\", \"Recall\", \"F1_Score\", \"Num_Masks\", \"Num_Masks_Label\"])\n",
    "\n",
    "# Fichier CSV pour les mesures des masques\n",
    "csv_masks_file = os.path.join(output_dir, \"mask_measurements.csv\")\n",
    "with open(csv_masks_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"Mask_ID\", \"Centroid_X\", \"Centroid_Y\", \"Area\", \"Equivalent_Diameter\"])\n",
    "\n",
    "size_threshold = 10000  # Seuil pour filtrer les objets trop grands\n",
    "\n",
    "# Variables pour les moyennes\n",
    "total_miou = total_precision = total_recall = total_f1 = total_images = 0\n",
    "\n",
    "for i, (batch, mask) in enumerate(dataloader):\n",
    "        print(f\"Image {i+1}/{len(dataloader)} - Avancement : {i/len(dataloader):.2%}\", end='\\r')\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        for idx, (image, label) in enumerate(zip(batch, mask)):\n",
    "\n",
    "            image_np = image.cpu().numpy()\n",
    "            pred = model.generate(image_np)\n",
    "            \n",
    "            res_tensor = torch.stack([torch.tensor(m['segmentation'], dtype=torch.bool) for m in pred])\n",
    "            filtered_tensor = res_tensor[res_tensor.sum(dim=(1, 2)) <= size_threshold]\n",
    "            res_merge = filtered_tensor.any(dim=0).to(device)\n",
    "            \n",
    "            label_np = np.array(label)\n",
    "            trach = torch.as_tensor(label_np[:, :, 2], dtype=torch.bool, device=device) if label_np.ndim == 3 and label_np.shape[-1] >= 3 else torch.as_tensor(label_np, dtype=torch.bool, device=device)\n",
    "            \n",
    "            num_masks_label = (label_np > 0).sum()\n",
    "            \n",
    "            # Calcul des m√©triques\n",
    "            img_miou = miou(res_merge, trach).item()\n",
    "            img_precision = precision(res_merge, trach).item()\n",
    "            img_recall = recall(res_merge, trach).item()\n",
    "            img_f1 = f1_score(res_merge, trach).item()\n",
    "            \n",
    "            total_miou += img_miou\n",
    "            total_precision += img_precision\n",
    "            total_recall += img_recall\n",
    "            total_f1 += img_f1\n",
    "            total_images += 1\n",
    "            \n",
    "            # Sauvegarde des r√©sultdataloader\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([f\"Image_{i}_{idx}\", img_miou, img_precision, img_recall, img_f1, filtered_tensor.shape[0], num_masks_label])\n",
    "            \n",
    "            # Mesure des aires et centro√Ødes des masques\n",
    "            for mask_id, mask_pred in enumerate(filtered_tensor):\n",
    "                mask_np = mask_pred.cpu().numpy().astype(np.uint8)\n",
    "                num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_np, connectivity=8)\n",
    "                \n",
    "                with open(csv_masks_file, mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    for j in range(1, num_labels):\n",
    "                        x, y = centroids[j]\n",
    "                        area = stats[j, cv2.CC_STAT_AREA]\n",
    "                        equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
    "                        writer.writerow([f\"Image_{i}_{idx}\", mask_id, x, y, area, equivalent_diameter])\n",
    "            \n",
    "            # Sauvegarde des images\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Pr√©diction\")\n",
    "            plt.imshow(res_merge.cpu().numpy(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Label\")\n",
    "            plt.imshow(trach.cpu().numpy(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.savefig(os.path.join(output_dir, f\"Image_{i}_{idx}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "if total_images > 0:\n",
    "    mean_miou = total_miou / total_images\n",
    "    mean_precision = total_precision / total_images\n",
    "    mean_recall = total_recall / total_images\n",
    "    mean_f1 = total_f1 / total_images\n",
    "    \n",
    "    print(f\"mIoU moyen: {mean_miou:.4f}\")\n",
    "    print(f\"Pr√©cision moyenne: {mean_precision:.4f}\")\n",
    "    print(f\"Rappel moyen: {mean_recall:.4f}\")\n",
    "    print(f\"F1-Score moyen: {mean_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le de seullage avec Opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Variables pour les moyennes\u001b[39;00m\n\u001b[1;32m     35\u001b[0m total_miou \u001b[38;5;241m=\u001b[39m total_precision \u001b[38;5;241m=\u001b[39m total_recall \u001b[38;5;241m=\u001b[39m total_f1 \u001b[38;5;241m=\u001b[39m total_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (image, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mimg_files[idx]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchmetrics\n",
    "import cv2\n",
    "\n",
    "# D√©finition du device\n",
    "device = \"cuda\"\n",
    "\n",
    "# D√©finition des m√©triques\n",
    "miou = torchmetrics.JaccardIndex(task='binary', num_classes=1).to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "f1_score = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/Annotations/Seuillage\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fichier CSV pour les m√©triques globales\n",
    "csv_file = os.path.join(output_dir, \"seuillage_results.csv\")\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"IoU\", \"Precision\", \"Recall\", \"F1_Score\", \"Num_Masks\", \"Num_Masks_Label\"])\n",
    "\n",
    "# Fichier CSV pour les mesures des masques\n",
    "csv_masks_file = os.path.join(output_dir, \"mask_measurements.csv\")\n",
    "with open(csv_masks_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"Mask_ID\", \"Centroid_X\", \"Centroid_Y\", \"Area\", \"Equivalent_Diameter\"])\n",
    "\n",
    "# Variables pour les moyennes\n",
    "total_miou = total_precision = total_recall = total_f1 = total_images = 0\n",
    "\n",
    "for idx, (image, mask) in enumerate(dataloader):\n",
    "    print(f\"{idx}/{len(dataloader)}\", end='\\r')\n",
    "    img_name = dataset.img_files[idx].split(\"/\")[-1]\n",
    "    \n",
    "    image = image.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    mask = mask.squeeze(0).cpu().numpy()\n",
    "    mask = mask[:,:,2] # keep only tracheid labels\n",
    "    \n",
    "    gray = np.mean(image, axis=2) if image.ndim == 3 else image\n",
    "    _, binary = cv2.threshold(gray.astype(np.uint8), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    processed = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "    processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(processed, connectivity=8)\n",
    "    \n",
    "    mask_bin = (mask > 0).astype(np.uint8)\n",
    "    \n",
    "    processed_bin = (processed > 0).astype(np.uint8)\n",
    "    \n",
    "    res_merge = torch.tensor(processed_bin, dtype=torch.bool, device=device)\n",
    "    \n",
    "    trach = torch.tensor(mask_bin, dtype=torch.bool, device=device)\n",
    "    \n",
    "    img_miou = miou(res_merge, trach).item()\n",
    "    img_precision = precision(res_merge, trach).item()\n",
    "    img_recall = recall(res_merge, trach).item()\n",
    "    img_f1 = f1_score(res_merge, trach).item()\n",
    "    \n",
    "    total_miou += img_miou\n",
    "    total_precision += img_precision\n",
    "    total_recall += img_recall\n",
    "    total_f1 += img_f1\n",
    "    total_images += 1\n",
    "    \n",
    "    num_masks_label = (mask_bin > 0).sum()\n",
    "    \n",
    "    # Sauvegarde des mesures des 3, 640, 640)masques\n",
    "    with open(csv_masks_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(1, num_labels):\n",
    "            x, y = centroids[i]\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
    "            writer.writerow([img_name, i, x, y, area, equivalent_diameter])\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Image d'origine\")\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Pr√©diction\")\n",
    "    pred_image = res_merge.cpu().numpy()\n",
    "    for centroid in centroids:\n",
    "        plt.scatter(centroid[0], centroid[1], color='red', s=50, marker='x')\n",
    "    plt.imshow(pred_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Label\")\n",
    "    plt.imshow(trach.cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f\"Image_{idx}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "if total_images > 0:\n",
    "    print(f\"mIoU moyen: {total_miou / total_images:.4f}\")\n",
    "    print(f\"Pr√©cision moyenne: {total_precision / total_images:.4f}\")\n",
    "    print(f\"Rappel moyen: {total_recall / total_images:.4f}\")\n",
    "    print(f\"F1-Score moyen: {total_f1 / total_images:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB3_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB3_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB3_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB4_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB4_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/11478/bandes_images/X200_11478_PB4_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB5_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB5_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB5_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB4_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB4_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB4_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB3_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB3_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/13823/bandes_images/X200_13823_PB3_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB5_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB5_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB5_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB3_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB3_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB3_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB6_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB6_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB6_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB4_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB4_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB4_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15485/bandes_images/X200_15485_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/15492/bandes_images/X200_15492_PB2_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV4D_B-P_1.tif : Image size (395460844 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4E_B-P_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4E_B-P_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4E_B-P_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4A_B_P_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4A_B_P_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4A_B_P_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4F_B_P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4F_B_P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4F_B_P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4D_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4D_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4D_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4B_B-P_1_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4C_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4C_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV4/bandes_images/X200_TGV4C_B-P_2_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV4C_B-P_1.tif : Image size (444658205 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_2_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_2_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_2_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5D_B-P_1_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV5B_B-P_2.tif : Image size (396766050 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5C_B-P_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5C_B-P_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5C_B-P_ech3.tif\n",
      "Bande ech1 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_1_ech1.tif\n",
      "Bande ech2 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_1_ech2.tif\n",
      "Bande ech3 extraite et enregistr√©e : /home/killian/data2025/TGV5/bandes_images/X200_TGV5A_B-P_1_ech3.tif\n",
      "Erreur lors du traitement de X200_TGV5B_B-P_1.tif : Image size (421885968 pixels) exceeds limit of 357913940 pixels, could be decompression bomb DOS attack.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS>=178956970\n",
    "\n",
    "extraire_bandes_images('/home/killian/data2025/11478')\n",
    "extraire_bandes_images('/home/killian/data2025/13823')\n",
    "extraire_bandes_images('/home/killian/data2025/15485')\n",
    "extraire_bandes_images('/home/killian/data2025/15492')\n",
    "extraire_bandes_images('/home/killian/data2025/TGV4')\n",
    "extraire_bandes_images('/home/killian/data2025/TGV5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "# import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,input_raster, tile_size=640, stride=10,transforms=None):\n",
    "\n",
    "        # raster = rasterio.open(input_raster).read()\n",
    "        # raster = plt.imread(input_raster)\n",
    "        raster = tifffile.imread(input_raster)\n",
    "        \n",
    "        self.raster = raster[:,:tile_size,:]\n",
    "        print(self.raster.shape)\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.raster.shape[0]/self.tile_size)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx == 0:\n",
    "            start_y = 0\n",
    "            end_y = self.tile_size\n",
    "        \n",
    "        else:\n",
    "            start_y = idx*self.tile_size - self.stride\n",
    "            end_y = (idx+1)*self.tile_size - self.stride\n",
    "\n",
    "        data = self.raster[start_y:end_y,:,:]\n",
    "        data = data.swapaxes(0,2)\n",
    "        # data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "        # if self.transforms:\n",
    "        #     data = self.transforms(data)\n",
    "        return data\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8935, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = CustomDataset('/home/killian/data2025/15485/X200_15485_PB1.tif', tile_size=640, stride=10)\n",
    "dataloader = DataLoader(dataset,batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plop\n",
      "(3, 640, 640)Avancement : 0.00%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 7.69%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 15.38%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 23.08%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 30.77%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 38.46%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 46.15%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 53.85%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640)Avancement : 61.54%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640) Avancement : 69.23%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640) Avancement : 76.92%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640) Avancement : 84.62%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 640, 640) Avancement : 92.31%\n",
      "255\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/sam2/inferences\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print('plop')\n",
    "\n",
    "# Boucle d'inf√©rence\n",
    "for i, image in enumerate(dataloader):\n",
    "    print(f\"Image {i+1}/{len(dataloader)} - Avancement : {i/len(dataloader):.2%}\", end='\\r')\n",
    "\n",
    "   \n",
    "    # image = torch.tensor(image, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Inf√©rence avec le mod√®le\n",
    "    # image_np = image.squeeze(0).swapaxes(0, 2).cpu().numpy()\n",
    "    image_np = image.squeeze(0).cpu().numpy()\n",
    "    # print(image_np.shape)\n",
    "    # print(image_np.max())\n",
    "    \n",
    "    image_np = image_np.swapaxes(0,2)\n",
    "    # image_np = np.asarray(image_np.astype(np.float32))\n",
    "    # print(type(image_np))\n",
    "    # image_pil = Image.fromarray(image_np)\n",
    "    # print(type(image_np))\n",
    "    pred = model.generate(image_np)\n",
    "    # Fusion des masques (si applicable)\n",
    "    res_tensor = torch.stack([torch.tensor(m['segmentation'], dtype=torch.bool) for m in pred])\n",
    "    res_merge = res_tensor.any(dim=0)\n",
    "    \n",
    "    # Sauvegarde des images\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(res_merge.cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, f\"Image_{i}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
