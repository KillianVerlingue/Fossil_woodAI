{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des d√©pendences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "# checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "# checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "checkpoint = \"/home/killian/sam2/checkpoints/sam2.1_hiera_small.pt\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "# model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "# predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n",
    "\n",
    "sam2 = build_sam2(model_cfg, checkpoint, device=\"cuda\", apply_postprocessing=False)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    # predictor.set_image(\"/home/killian/sam2\")\n",
    "    # masks, _, _ = predictor.predict(\"0\")\n",
    "    # üîπ Chemins vers le mod√®le et la configuration\n",
    "#checkpoint = \"./checkpoints/sam2_hiera_large.pt\"\n",
    "#model_cfg = \"sam2_hiera_l.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on d√©finit le DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "# import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,input_raster, tile_size=640, stride=10,transforms=None):\n",
    "\n",
    "        # raster = rasterio.open(input_raster).read()\n",
    "        # raster = plt.imread(input_raster)\n",
    "        raster = tifffile.imread(input_raster)\n",
    "        self.raster = raster[:tile_size,:,:,]\n",
    "        print(self.raster.shape)\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.raster.shape[0]/self.tile_size)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx == 0:\n",
    "            start_x = 0\n",
    "            end_x = self.tile_size\n",
    "            \n",
    "        else:\n",
    "            start_x = idx*self.tile_size - self.stride\n",
    "            end_x = (idx+1)*self.tile_size - self.stride\n",
    "\n",
    "        data = self.raster[:, start_x:end_x, :]\n",
    "        data = np.transpose(data, (1, 2, 0))  # Passe de (C, H, W) √† (H, W, C)\n",
    "        # data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "        # if self.transforms:\n",
    "        #     data = self.transforms(data)\n",
    "        return data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader et dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 12554, 3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = CustomDataset('/home/killian/data2025/15485/X200_15485_PB1.tif', tile_size=640, stride=10)\n",
    "dataset = CustomDataset('/home/killian/data2025/TGV4/X200_TGV4A_B_P.tif', tile_size=640, stride=10)\n",
    "dataloader = DataLoader(dataset,batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparametres mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametres\n",
    "model = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    points_per_side=20,  # Plus de points pour capturer les details\n",
    "    points_per_batch=30,  # Augmenter pour calculer le nbre de points pris en meme temps (/!\\ GPU)\n",
    "    pred_iou_thresh=0.7,  # Reduire pour accepter plus de mask\n",
    "    stability_score_thresh=0.70,  # Rzduire pour ne pas exclure trop de mask\n",
    "    stability_score_offset=0.8,\n",
    "    crop_n_layers=2,  # Ammeliore la segmentation des petites structures\n",
    "    box_nms_thresh=0.70,  # Eviter la suppression excessive de petite structure\n",
    "    crop_n_points_downscale_factor=1.5,  # Adapter aux images a haute resolution\n",
    "    min_mask_region_area=15.0,  # Conserver plus de petits objets\n",
    "    use_m2m=True,  # Mode avanc√© \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 640, 3)vancement : 0.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"/home/killian/sam2/inferences\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fichiers CSV pour sauvegarder les masks\n",
    "csv_masks_file = os.path.join(output_dir, \"mask_measurements.csv\")\n",
    "\n",
    "with open(csv_masks_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image_ID\", \"Mask_ID\", \"Centroid_X\", \"Centroid_Y\", \"Area\", \"Equivalent_Diameter\"])\n",
    "\n",
    "size_threshold = 560  # Seuil pour filtrer les objets trop grands\n",
    "# (0.5060 px/¬µm) donc <280¬µm pour\n",
    "\n",
    "# Boucle d'inf√©rence\n",
    "for i, image in enumerate(dataloader):\n",
    "    print(f\"Image {i+1}/{len(dataloader)} - Avancement : {i/len(dataloader):.2%}\", end='\\r')\n",
    "    \n",
    "    image_np = image.squeeze(0).cpu().numpy()\n",
    "    image_np = image_np.swapaxes(1, 2)\n",
    "    print(image_np.shape)\n",
    "    pred = model.generate(image_np)\n",
    "    \n",
    "    res_tensor = torch.stack([torch.tensor(m['segmentation'], dtype=torch.bool) for m in pred])\n",
    "    filtered_tensor = res_tensor[res_tensor.sum(dim=(1, 2)) <= size_threshold]\n",
    "    res_merge = filtered_tensor.any(dim=0).to(device)\n",
    "    \n",
    "    # Trier les masques de droite √† gauche selon la coordonn√©e X de leur centro√Øde\n",
    "    masks_info = []\n",
    "    for mask_id, mask_pred in enumerate(filtered_tensor):\n",
    "        mask_np = mask_pred.cpu().numpy().astype(np.uint8)\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_np, connectivity=8)\n",
    "        \n",
    "        for j in range(1, num_labels):  # On ignore le label 0 (arri√®re-plan)\n",
    "            x, y = centroids[j]\n",
    "            area = stats[j, cv2.CC_STAT_AREA]\n",
    "            equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
    "            masks_info.append((x, y, area, equivalent_diameter, mask_id))\n",
    "            \n",
    "    # Trier les masques en fonction de X (croissant)\n",
    "    masks_info.sort(reverse=True, key=lambda m: m[0])\n",
    "    \n",
    "    # Sauvegarde des masques tri√©s\n",
    "    with open(csv_masks_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for x, y, area, equivalent_diameter, mask_id in masks_info:\n",
    "            writer.writerow([f\"Image_{i}\", mask_id, x, y, area, equivalent_diameter])\n",
    "    \n",
    "    # Sauvegarde de l'image de pr√©diction et de l'image originale\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(\"Image Originale\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(res_merge.cpu().numpy(), cmap='gray')\n",
    "    plt.title(\"Masques Pr√©dits\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f\"Image_{i}.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
